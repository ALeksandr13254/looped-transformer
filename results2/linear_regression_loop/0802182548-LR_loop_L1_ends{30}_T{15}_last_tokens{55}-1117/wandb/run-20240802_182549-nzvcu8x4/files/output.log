  0%|                                                                                                                                                    | 0/20001 [00:00<?, ?it/s]C:\Users\artem\Music\looped_transformer-main\scripts\nano_gpt.py:79: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
  y = torch.nn.functional.scaled_dot_product_attention(
loss 9.856182098388672:   0%|▏                                                                                                                  | 27/20001 [00:00<09:03, 36.72it/s]
number of parameters: 0.79M
train from scratch









































































































































































































































































































































































































































































































































loss 4.719381332397461: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20001/20001 [17:27<00:00, 19.10it/s]