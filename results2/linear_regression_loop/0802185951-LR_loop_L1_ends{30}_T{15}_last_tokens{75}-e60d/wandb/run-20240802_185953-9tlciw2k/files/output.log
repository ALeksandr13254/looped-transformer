  0%|                                                                                                                                                    | 0/20001 [00:00<?, ?it/s]C:\Users\artem\Music\looped_transformer-main\scripts\nano_gpt.py:79: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
  y = torch.nn.functional.scaled_dot_product_attention(
loss 10.16615104675293:   0%|▏                                                                                                                  | 22/20001 [00:00<09:40, 34.43it/s]
number of parameters: 0.79M
train from scratch













































































































































































































































































































































































































































































































































































loss 2.2351856231689453: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20001/20001 [18:38<00:00, 17.88it/s]