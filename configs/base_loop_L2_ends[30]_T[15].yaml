wandb:
    project: Lets_Loop2
    notes:
    log_every_steps: 100

gpu:
    cuda: True
    n_gpu: 0

model:
    family: gpt2_loop
    n_embd: 256
    n_layer: 2
    n_head: 8
    n_dims: 20
    n_positions: 101
    n_last_tokens: -1
    use_lstm_layer: False

training:
    batch_size: 64
    task_name: linear_regression
    learning_rate: 0.0001
    weight_decay: 0.0
    train_steps: 20001
    save_every_steps: 1000
    keep_every_steps: 20000
    curriculum:
        dims:
            start: 10
            end: 10
            inc: 1
            interval: 20000
        points:
            start: 11
            end: 41
            inc: 2
            interval: 1000
        loops:
            start: 15
            end: 30
            inc: 2
            interval: 500
    n_loop_window: 15

out_dir: ./results2/linear_regression_loop
debug_mode: False
